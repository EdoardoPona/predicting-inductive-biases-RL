# NOT USED. this is reimplemented in RL4LMs.

from typing import Dict, Any

from rl4lms.envs.text_generation.observation import Observation
from rl4lms.envs.text_generation.reward import RewardFunction

# makes the reward functions. gets imported if you need a reward function.

# these all take Observation objects as input when called! see observation.py in the RL4LMs repo.


# The toy tasks in the paper:
# (prev_observation is the prompt, current_observation is None since the episode ends instantly, and done = True since the episode ends instantly)
# I am assuming action is simply a string of the outputted numbers
# I am assuming prev_observation.meta_info = { 'label' : 0 (true feature absent) or 1 (true feature present) } (this data is in the data frames generated by make_data in toy.py)
# It seems to me all 4 toy tasks in the paper naturally have the same reward function

class LoveringToyTaskRewardFunction(RewardFunction):
    # The task is to output zeros if the true feature is present and ones if it isn't

    def __call__(self, prev_observation: Observation,
                action: str,
                current_observation: Observation = None,
                done: bool = True,
                meta_info: Dict[str, Any] = None) -> float:
        if prev_observation.meta_info['label']==0:
            reward = -sum([abs(int(i)) for i in action])
        else:
            reward = -sum([abs(int(i)-1) for i in action])
        return reward
        

